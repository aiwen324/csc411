\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\begin{document}

% ------------------------- Part 1 -------------------------
\section{Part 1}
The dataset is a dictionary consisting of 9 training sets which are just number 0 to 9 and 9 test sets. Each training set is a matrix with $N\times 784$ which N represents the number of handwritten digit images in the dataset and each row is an array of size $1\times 784$ is the flatten array of handwritten digit image, which originally is $28\times 28$.

I randomly pick 10 images for each number, they all looks pretty good. At least I believe they are classified to the correct number.

% TODO: Include the image
% ------------------------- Part 2 -------------------------
\section{Part 2}
\begin{lstlisting}[language=Python]
    def layer_computation(x, W, b, act_func=lambda x: x):
        L = np.dot(W.T, x) + b
      	output = softmax(L)
      	return L, output
\end{lstlisting}

% ------------------------- Part 3 -------------------------
Suppose X is a matrix with size $N\times M$, where N is the number of input layer, M is the number of training cases. Let W be a matrix be $N\times L$ where L is the number of units in hidden layer. Here L is 10. The output matrix Y has size of $L\times M$
\begin{align}
    % \frac{\partial \mathcal{L}_{CE}}{\partial y_k} &= \frac{-t_k}{y_k}\\
    % \mathcal{L}_{CE}(y, t) &= -t^T(logy)\\
    %                        &= -\sum_{j=1}^K t_j logy_j\\
    %                        &= -\sum_{j=1}^K t_j log\frac{e^{o_j}}{\sum_i e^{o_i}}\\
    %                        &= -\sum_{j=1}^K t_j (loge^{o_j} - log\sum_i e^{o_i})\\
    %                        &= -\sum_{j=1}^K t_j (o_j - log\sum_i e^{o_i})\\
    % \frac{\partial \mathcal{L}_{CE}}{\partial o_k} 
    % &= -t_k + \frac{e^{o_k}}{\sum_i e^{o_i}}\sum_{j=1}^K t_j && \text{Notice $\sum_{j=1}^K t_j = 1$}\\
    % &= -t_k + \frac{e^{o_k}}{\sum_i e^{o_i}}\\
    % &= -t_k + y_k\\
    % &= y_k - t_k
    y_j^{(i)} &= \frac{e^{o_j^{(i)}}}{\sum_k e^{o_k^{(i)}}}\\
    o_k^{(i)} &= \sum_{p=1}^{N}w_{pk}x_p^{(i)}\\
    \mathcal{L}_{CE} 
    &= \sum_{i=1}^M \sum_{j=0}^{L-1} -t_j^{(i)}logy_j^{(I)}\\
    &= \sum_{i=1}^M \sum_{j=0}^{L-1} -t_j^{(i)}[o_j^{(i)}-log(\sum_{k=0}^{L-1} e^{o_k^{(i)}})]\\
    &= \sum_{i=1}^M \sum_{j=0}^{L-1} -t_j^{(i)}[\sum_{p=1}^{N}w_{pj}x_p^{(i)} - log\sum_k (e^{\sum_{p=1}^{N}w_{pk}x_p^{(i)}})]\\
    \frac{\partial \mathcal{L}_{CE}}{\partial w_{sm}}
    &= \sum_{i=1}^M [(-t_m^{(i)} + \frac{e^{\sum_{s=1}^{N}w_{sm}x_s^{(i)}}}{\sum_k (e^{\sum_{p=1}^{N}w_{pk}x_p^{(i)}})}\sum_{j=0}^{L-1} t_j^{(i)})\,\,x_s^{(i)}] && \text{Notice $\sum_{j=1}^{L-1} t_j = 1$}\\
    &= \sum_{i=1}^M [(-t_m^{(i)} + \frac{e^{o_m^{(i)}}}{\sum_k e^{o_k^{(i)}}})x_s^{(i)}]\\
    &= \sum_{i=1}^M [(-t_m^{(i)} + y_m^{(i)})x_s^{(i)}]
\end{align}
Therefore, we get the derivative of weight is:
\[
    \frac{\partial \mathcal{L}_{CE}}{\partial W} = X(Y-T)^T
\]

\end{document}